{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637912c7",
   "metadata": {},
   "source": [
    "Import de la bibliothèque Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6251077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf2dae",
   "metadata": {},
   "source": [
    "Téléchargement du modèle Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad91df1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mollama\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpull\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mllama3.1:8b\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ollama\\_client.py:459\u001b[39m, in \u001b[36mClient.pull\u001b[39m\u001b[34m(self, model, insecure, stream)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpull\u001b[39m(\n\u001b[32m    448\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    449\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    452\u001b[39m   stream: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    453\u001b[39m ) -> Union[ProgressResponse, Iterator[ProgressResponse]]:\n\u001b[32m    454\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    455\u001b[39m \u001b[33;03m  Raises `ResponseError` if the request could not be fulfilled.\u001b[39;00m\n\u001b[32m    456\u001b[39m \n\u001b[32m    457\u001b[39m \u001b[33;03m  Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mProgressResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/pull\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPullRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m      \u001b[49m\u001b[43minsecure\u001b[49m\u001b[43m=\u001b[49m\u001b[43minsecure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ollama\\_client.py:189\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    187\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ollama\\_client.py:135\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    133\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mConnectionError\u001b[39m: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download"
     ]
    }
   ],
   "source": [
    "ollama.pull('llama3.1:8b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee9bc0",
   "metadata": {},
   "source": [
    "Génération de texte avec la fonction generate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23025817",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=ollama.generate(model = 'llama3.1:8b',\n",
    "    prompt='Give me a joke on Generative AI',\n",
    "    )\n",
    "\n",
    "print(result['response']) #afficher un seul message "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3ae9bf",
   "metadata": {},
   "source": [
    "Génération de texte en mode conversation (chat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd09a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat( model='llama3.1:8b',messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Give me a joke on Generative AI'\n",
    "    }\n",
    "]\n",
    ")\n",
    "print(response['message']['content']) #supporte plusieurs messages dans une conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0f2f18",
   "metadata": {},
   "source": [
    "Création du modèle Jarvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f2820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfiles='''\n",
    "FROM llama3.1:8b\n",
    "SYSTEM You are Jarvis form Iron and the user is Tony Stark.\n",
    "'''\n",
    "\n",
    "ollama.create(model='jarvis2', modelfile=modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd0aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.delete('jarvis2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2b16ac",
   "metadata": {},
   "source": [
    "Rest API : \n",
    "Interaction avec un modèle via Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad20b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client #classe client permet de communiquer avec le serveur ollama \n",
    "client = Client(host= 'http://localhost:11434')\n",
    "response = client.chat(model='llama3.1:8b', messages=[{\"role\": \"user\", \"content\": \"Explain gravity to a 6 year old kid?\"}])\n",
    "\n",
    "response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72870f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client \n",
    "client = Client(host= 'http://localhost:11434')\n",
    "response = client.chat(model='llama3.1:8b', messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are Jarvis from Iron Man and the user is Tony Stark. Respond in only a single line.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "    ])\n",
    "\n",
    "response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e91ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "llm = OpenAI(base_url='http://localhost:11434', api_key='blank') #api key est vide car on utilise le serveur local d'ollama\n",
    "\n",
    "response = llm.chat.completions.create(\n",
    "    model='llama3.1:8b',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are Jarvis from Iron man and the user is Tony Stark. Respond in only a single line.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Good morning, Mr. Stark. Shall I proceed with your scheduler for today?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Yes, run the diagnostics and tell me the results in a single line.\"},\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb2f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain\n",
    "pip install langchain-core\n",
    "pip install langchain-Ollama\n",
    "pip install langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4532a5e",
   "metadata": {},
   "source": [
    "LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e9b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that gives a one line definition of the word entered by user.\"),\n",
    "    (\"user\", \" {word}?\"),\n",
    "])\n",
    "messages = chat_template.format_messages(word=\"Sesquipedalian\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f007e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm=ChatOllama(model=\"llama3.1:latest\",\n",
    "               temperature= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d6daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_message = llm.invoke(messages)\n",
    "ai_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain= chat_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e94db",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"word\":\"Onomatopoeia\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383a4ebe",
   "metadata": {},
   "source": [
    "RAG Application using Ollama and Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4711d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f99ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = TextLoader(\"./LangchainRetrieval.txt\").load()\n",
    "\n",
    "raw_documents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf57d6",
   "metadata": {},
   "source": [
    "Découper le texte en morceaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7565a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,   # nombre de caractères par chunk\n",
    "    chunk_overlap=20  # chevauchement entre chunks\n",
    ")\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "len(documents)\n",
    "print(documents[0])\n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d7346",
   "metadata": {},
   "source": [
    "Créer les embeddings avec Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9819569",
   "metadata": {},
   "outputs": [],
   "source": [
    "oembed = OllamaEmbeddings(base_url='http://localhost:11434', model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db86e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(documents, embedding=oembed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2137973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is text embedding and how does langchain help in doing it\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b8021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassThrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e818e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based on the following context: {context} \\n Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b100eaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"llama3.1:latest\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6956c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a779be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_doc(docs): #transforme les documents pertinents en un seul texte.\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever | format_doc, \"question\": RunnablePassThrough()} \n",
    "    | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a377469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"What is text embedding and how does langchain help in doing it\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
